import logging
import numpy as np
import spacy
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.exceptions import NotFittedError

class AnswerAssessment:
    def __init__(self, pipe):
        self.pipe = pipe
        self.grader = AutoGrader()
    def create_prompt(self, question, correct_concepts, student_answer):
      """Create a more structured prompt for LLM feedback"""
      return [
          {"role": "system", "content": """You are an educational assistant providing specific, constructive feedback.
          Analyze the student's response against the grading requirements and provide detailed feedback.

          Focus on:
          1. Content coverage and accuracy
          2. Key concept usage
          3. Depth of analysis
          4. Connection to requirements

          Format your response exactly as:

          Missing Concepts:
          - [List specific key concepts that are missing or need more development]

          Areas of Strength:
          - [List what the student did well]

          Suggestions for Improvement:
          - [2-3 specific, actionable suggestions]
          """},
          {"role": "user", "content": f"""
          Question: {question}

          Grading Requirements:
          {correct_concepts}

          Student Response:
          {student_answer}
          """}
      ]

    def assess_answer(self, question, correct_concepts, student_answer):
        """Enhanced assessment with better error handling"""
        try:
            # Get LLM feedback
            prompt = self.create_prompt(question, correct_concepts, student_answer)
            llm_response = self.pipe(prompt, max_new_tokens=1024)

            if not llm_response or not llm_response[0].get("generated_text"):
                raise ValueError("No feedback generated by LLM")

            # Parse LLM feedback
            llm_feedback = self._parse_llm_feedback(llm_response[0]["generated_text"])

            # Get automated grading
            parsed_reqs = self.grader.parse_requirements(correct_concepts)
            grade_result = self.grader.grade_response(parsed_reqs, student_answer)

            # Calculate composite score
            composite_score = self._calculate_composite_score(grade_result, llm_feedback)

            # Create complete feedback dictionary
            feedback = {
                'llm_feedback': llm_feedback,
                'automated_grading': grade_result,
                'composite_score': composite_score
            }

            return feedback

        except Exception as e:
            logging.error(f"Error in answer assessment: {e}")
            # Return a valid feedback structure even on error
            return {
                'error': str(e),
                'llm_feedback': {'missing_concepts': [], 'areas_of_strength': [], 'suggestions': []},
                'automated_grading': {'total_points': 0, 'max_points': 100, 'percentage': 0},
                'composite_score': 0
            }
    def _parse_llm_feedback(self, feedback_text):
        """Improved LLM feedback parsing with better error handling"""
        feedback_dict = {
            'missing_concepts': [],
            'areas_of_strength': [],
            'suggestions': []
        }

        # Handle case where feedback_text is a list
        if isinstance(feedback_text, list):
            # Extract the content from the last message if it exists
            if feedback_text and isinstance(feedback_text[-1], dict):
                feedback_text = feedback_text[-1].get('content', '')
            else:
                return feedback_dict

        # Convert to string if not already
        feedback_text = str(feedback_text)

        current_section = None

        for line in feedback_text.split('\n'):
            line = line.strip()

            if 'Missing Concepts:' in line:
                current_section = 'missing_concepts'
            elif 'Areas of Strength:' in line:
                current_section = 'areas_of_strength'
            elif 'Suggestions for Improvement:' in line:
                current_section = 'suggestions'
            elif line.startswith('- ') and current_section:
                feedback_dict[current_section].append(line[2:])

        return feedback_dict

    def _calculate_composite_score(self, grade_result, llm_feedback):
        """Calculate a weighted composite score with improved error handling"""
        try:
            automated_score = float(grade_result.get('percentage', 0))

            # Count missing concepts and strengths with safe access
            missing_concepts = llm_feedback.get('missing_concepts', [])
            strengths = llm_feedback.get('areas_of_strength', [])

            concept_penalty = len(missing_concepts) * -2  # -2 points per missing concept
            strength_bonus = len(strengths) * 2   # +2 points per strength

            composite_score = automated_score + concept_penalty + strength_bonus
            return max(0, min(100, composite_score))
        except Exception as e:
            logging.error(f"Error calculating composite score: {e}")
            return automated_score  # Fall back to automated score on error

class ImprovedAutoGrader(AutoGrader):
    def __init__(self):
        super().__init__()
        self.similarity_threshold = 0.6
        self.concept_match_threshold = 0.25

    def _calculate_concept_coverage(self, required_concepts, student_concepts):
        """Improved concept coverage calculation with fuzzy matching"""
        if not required_concepts:
            return 1.0

        matches = 0
        for req_concept in required_concepts:
            # Check for exact matches
            if req_concept in student_concepts:
                matches += 1
                continue

            best_match = max(
                (self._calculate_similarity(req_concept, student_concept)
                 for student_concept in student_concepts),
                default=0
            )

            if best_match > self.concept_match_threshold:
                matches += best_match

        return matches / len(required_concepts)

    def _analyze_response_depth(self, response, requirement):
        """Enhanced depth analysis with better metrics"""
        doc = self.nlp(response)

        # Calculate metrics
        num_sentences = len(list(doc.sents))
        avg_sent_length = len(doc) / num_sentences if num_sentences > 0 else 0

        # Analyze sentence complexity
        complex_sentences = sum(1 for sent in doc.sents if len(list(sent.root.children)) > 3)

        # Count supporting evidence
        evidence_markers = ['because', 'for example', 'such as', 'therefore', 'thus', 'consequently']
        evidence_count = sum(1 for token in doc if token.text.lower() in evidence_markers)

        # Normalized scores
        sent_score = min(1.0, num_sentences / 10)
        complexity_score = min(1.0, complex_sentences / num_sentences) if num_sentences > 0 else 0
        evidence_score = min(1.0, evidence_count / 5)

        # Weighted combination
        depth_score = (
            0.4 * sent_score +
            0.3 * complexity_score +
            0.3 * evidence_score
        )

        return depth_score

    def _generate_requirement_feedback(self, overall_score, concept_score, relevance_score, depth_score):
        """More detailed and actionable feedback generation"""
        feedback = []

        # Concept coverage feedback
        if concept_score < self.concept_match_threshold:
            severity = "significant" if concept_score < 0.4 else "some"
            feedback.append({
                'area': 'Key Concepts',
                'issue': f"Shows {severity} gaps in key concept coverage",
                'suggestion': "Focus on incorporating more specific terminology and concepts from the course material"
            })

        # Depth analysis feedback
        if depth_score < 0.7:
            if depth_score < 0.4:
                feedback.append({
                    'area': 'Analysis Depth',
                    'issue': "Response lacks sufficient depth and detail",
                    'suggestion': "Expand your analysis with specific examples and support your claims with evidence"
                })
            else:
                feedback.append({
                    'area': 'Analysis Depth',
                    'issue': "Some points could be developed further",
                    'suggestion': "Consider adding more supporting details and connecting your ideas more explicitly"
                })

        # Positive feedback
        strengths = []
        if concept_score >= 0.8:
            strengths.append("Excellent grasp of key concepts")
        elif concept_score >= 0.7:
            strengths.append("Good understanding of main concepts")

        if depth_score >= 0.8:
            strengths.append("Strong analytical depth")
        elif depth_score >= 0.7:
            strengths.append("Good level of detail and analysis")

        if strengths:
            feedback.append({
                'area': 'Strengths',
                'positive': ', '.join(strengths)
            })

        return feedback